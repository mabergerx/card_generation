{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "headers = {'X-Mashape-Key': api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be speaking to the Hearthstone API hosted by Mashape. It is free and let's us specify what card information we want to receive via paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"hh8DW29tdDmshbghN71NJPI0jgFzp1Ay5MzjsniIz9a8Piq7xp\"\n",
    "def get_cards_by_type(card_type, collectible=1, cost=None, durability=None, health=None, key=api_key):\n",
    "    endpoint_by_type = f'https://omgvamp-hearthstone-v1.p.mashape.com/cards/types/{card_type}'\n",
    "    payload = {'collectible': collectible, 'cost': cost, 'durability': durability, 'health': health}\n",
    "    r = requests.get(endpoint_by_type, params=payload, headers=headers)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'artist': 'Andrew Hou',\n",
       "  'cardId': 'UNG_004',\n",
       "  'cardSet': \"Journey to Un'Goro\",\n",
       "  'collectible': True,\n",
       "  'cost': 8,\n",
       "  'dbfId': '41130',\n",
       "  'faction': 'Neutral',\n",
       "  'flavor': 'Comes with fries and a drink.',\n",
       "  'img': 'http://media.services.zam.com/v1/media/byName/hs/cards/enus/UNG_004.png',\n",
       "  'imgGold': 'http://media.services.zam.com/v1/media/byName/hs/cards/enus/animated/UNG_004_premium.gif',\n",
       "  'locale': 'enUS',\n",
       "  'name': 'Dinosize',\n",
       "  'playerClass': 'Paladin',\n",
       "  'rarity': 'Epic',\n",
       "  'text': \"Set a minion's Attack and Health to 10.\",\n",
       "  'type': 'Spell'},\n",
       " {'artist': 'Anton Magdalina',\n",
       "  'cardId': 'UNG_854',\n",
       "  'cardSet': \"Journey to Un'Goro\",\n",
       "  'collectible': True,\n",
       "  'cost': 8,\n",
       "  'dbfId': '42009',\n",
       "  'faction': 'Neutral',\n",
       "  'flavor': 'Bingo! Minion DNA!',\n",
       "  'img': 'http://media.services.zam.com/v1/media/byName/hs/cards/enus/UNG_854.png',\n",
       "  'imgGold': 'http://media.services.zam.com/v1/media/byName/hs/cards/enus/animated/UNG_854_premium.gif',\n",
       "  'locale': 'enUS',\n",
       "  'mechanics': [{'name': 'Discover'}],\n",
       "  'name': 'Free From Amber',\n",
       "  'playerClass': 'Priest',\n",
       "  'rarity': 'Rare',\n",
       "  'text': '<b>Discover</b> a minion that costs (8) or more. Summon it.',\n",
       "  'type': 'Spell'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cards_by_type('Spell', cost=8)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = get_cards_by_type('Spell')\n",
    "weapons = get_cards_by_type('Weapon')\n",
    "minions = get_cards_by_type('Minion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm a fan of saving data offline in case I can't access the API / the API is down / my rates are exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cardtype, collection in zip(['minions', 'spells', 'weapons'], [minions, spells, weapons]):\n",
    "    with open(f'{cardtype}.json', 'w') as outfile:\n",
    "        json.dump(collection, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can load the data offline whenever we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/minions.json') as json_data:\n",
    "    minions = json.load(json_data)\n",
    "    \n",
    "with open('data/spells.json') as json_data:\n",
    "    spells = json.load(json_data)\n",
    "    \n",
    "with open('data/weapons.json') as json_data:\n",
    "    weapons = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now separate the titles, flavors, texts, mechanics, costs and stats.\n",
    "#### Each card type has different attributes and design logic behind them, so we want to make educated splits. Moreover, not all fields are of interest for us for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_attributes = {\"name\", \"cardId\", \"cost\", \"img\", \"playerClass\", \"rarity\", \"text\", \"flavor\", \"mechanics\"}\n",
    "\n",
    "minion_attributes = {\"name\", \"cardId\", \"cost\", \"health\", \"attack\", \"img\", \"playerClass\", \"rarity\", \"text\", \"flavor\", \"mechanics\"}\n",
    "\n",
    "weapon_attributes = {\"name\", \"cardId\", \"cost\", \"durability\", \"attack\", \"img\", \"playerClass\", \"rarity\", \"text\", \"flavor\", \"mechanics\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this first generation example we shall funnel all the cards to the above attributes to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_card(card, attrs):\n",
    "    \n",
    "    concise = {a: card[a] if a in card else None for a in attrs}\n",
    "    \n",
    "    return concise\n",
    "\n",
    "spells_concise = [normalize_card(spell_card, spell_attributes) for spell_card in spells]\n",
    "minions_concise = [normalize_card(minion_card, minion_attributes) for minion_card in minions]\n",
    "weapons_concise = [normalize_card(weapon_card, weapon_attributes) for weapon_card in weapons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is now quite ready to be modelled! We can now access the needed fields directly from the correpsonding cardtype set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small LSTM\n",
    "\n",
    "### Let's get all the card texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cardtexts():\n",
    "    \n",
    "    weapon_texts = [card['text'] for card in weapons if 'text' in card]\n",
    "    minion_texts = [card['text'] for card in minions if 'text' in card]\n",
    "    spell_texts = [card['text'] for card in spells if 'text' in card] \n",
    "    \n",
    "    return weapon_texts + minion_texts + spell_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our average sequence length is ... characters. This is handy to know for the LSTM sequence length parameter. Let's round it down for sake of memorability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_sequence_length = np.mean([len(list(text)) for text in all_cardtexts])\n",
    "# average_sequence_length\n",
    "# SEQUENCE_LENGTH = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our generative model is a character based one, so our input data is a huge list of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_text_chars = [ch for one_sentence in all_cardtexts for ch in list(one_sentence)]\n",
    "all_text_unique_chars = list(set(all_text_chars))\n",
    "all_text_unique_chars.sort() # THIS IS VERY IMPORTANT TO GET REPRODUCIBLE RESULTS WHEN SAVING THE MODEL, \n",
    "                             # OTHERWISE THE CHARACTER MAPPING WILL BE DIFFERENT EACH TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Data length: {len(all_text_chars)} characters')\n",
    "# print(f'Vocabulary size: {len(all_text_unique_chars)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The unique characters are the features for our model. Let's numerify them to make them ML ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix_to_char = {ix:char for ix, char in enumerate(all_text_unique_chars)}\n",
    "# char_to_ix = {char:ix for ix, char in enumerate(all_text_unique_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM expects input of the shape (batch_size, length_of_sequence, number_features)\n",
    "#### batch_size: amount of sequences which are fed into the network at a single weight update iteration, just as in a regular feedforward neural network\n",
    "#### length_of_sequence: the amount of \"neural networks\", the memory or the amount of steps the network looks at at each step. In our example, we want to predict a character given 57 previous characters.\n",
    "#### number_features: the length of one featurized element. In the case of images it could be padded standardized vectors of pixels. In case of text it is the length of our vocab, because our input is going to be represented by every char in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_FEATURES = len(all_text_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will initialize our X and y for the network by creating a correctly shaped zeros array which we will fill iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.zeros((int(len(all_text_chars)/SEQUENCE_LENGTH), SEQUENCE_LENGTH, NUMBER_FEATURES))\n",
    "# y = np.zeros((int(len(all_text_chars)/SEQUENCE_LENGTH), SEQUENCE_LENGTH, NUMBER_FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea is that we want to predict the same length sequence as is our input, BUT just shift it with one character. This is common while using LSTM's for prediction as it makes shaping the data convenient.\n",
    "### Then we practically are predicting one character at a time. For a sequence [c, a, r, r, o] we want to have the sequence [a, r, r, o, t] as output. This way, each char in the input has the following char as label (c -> a, a -> r, r -> r, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(int(len(all_text_chars)/SEQUENCE_LENGTH)):\n",
    "#     X_sequence = all_text_chars[i*SEQUENCE_LENGTH:(i+1)*SEQUENCE_LENGTH]  #Get next sequence of length 57 as input.\n",
    "#     X_sequence_ix = [char_to_ix[value] for value in X_sequence]  # Convert the above sequence to the integer mapping.\n",
    "#     # TODO: make this one hot encoding differently: Keras or sklearn or something.\n",
    "#     input_sequence = np.zeros((SEQUENCE_LENGTH, NUMBER_FEATURES))  # Create a skeleton for the input sequence: we create a 2d numpy matrix which has a feature array of 94 \n",
    "#                                                                    # long for each of the 57 characters in sequence. This way we basically one hot encode our sequences. \n",
    "#     for j in range(SEQUENCE_LENGTH):  # The one hot encoding process: we replace a zero with a one on a position in the input sequence which corresponds with the index of a character in our converted array!\n",
    "#         input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "#     X[i] = input_sequence  #For each spot in X (which stands for each sequence) we fill in our one hot encoded array!\n",
    "    \n",
    "#     #Same for y!\n",
    "#     y_sequence = all_text_chars[i*SEQUENCE_LENGTH+1:(i+1)*SEQUENCE_LENGTH+1]\n",
    "#     y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "#     target_sequence = np.zeros((SEQUENCE_LENGTH, NUMBER_FEATURES))\n",
    "#     for j in range(SEQUENCE_LENGTH):\n",
    "#         target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "#     y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model hyperparameters: 3 LSTM layers with each having 500 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 500\n",
    "LAYER_NUM = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture (3 LSTM layers with Dropout regularization), a TimeDistributed layer which makes sure all of our LSTM cell states are passed through the Dense layer and not just the last one because the goal is to predict a sequence and not just the last character. Otherwise the loss function will only be calculated for the last input and not the whole sequence.\n",
    "### Because we are essentially predicting classes (characters are categorical) we use categorical crossentropy as our loss function and rmsprop proved to be a good optimizer for RNN tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, NUMBER_FEATURES), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(NUMBER_FEATURES)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, ixtochars):\n",
    "    \n",
    "    hele_tekst = []\n",
    "    \n",
    "    ix = [np.random.randint(NUMBER_FEATURES)]  # We start with a random character.\n",
    "    y_char = [ixtochars[ix[0]]]  # Get the random char's string variant.\n",
    "    \n",
    "    X = np.zeros((1, length, NUMBER_FEATURES)) # Generate a placeholder empty numpy array which contains an zeros array for each of the amount of letters \n",
    "                                                # that we want to generate and each of these arrays is as long as is our vocabulary, just as in the data \n",
    "                                                # prep step.\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[0]] = 1\n",
    "        print(ixtochars[ix[0]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1) # Given a single character one-hot encoding, generate predictions for the next one \n",
    "                                                            # and choose the best one.\n",
    "        hele_tekst.append(ixtochars[ix[0]])  # Append the predicted character to the list, repeat it for length steps.\n",
    "    return hele_tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We specify the batch size (just the same as in a simple Feedforward ANN) and the length of card text we want to generate. \n",
    "### In an endless loop, we fit the model for one epoch, so we let the network go through the whole data with batch size 50, generate the text to just see intermediate steps of the network and then fit again on the same model but now with different batch. \n",
    "### We also want to save weights of the model every 200 epochs to be able to compare the performance / load weights into the same architecture later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 0\n",
    "BATCH_SIZE = 50\n",
    "GENERATE_LENGTH = 57\n",
    "while True:\n",
    "    print('\\n')\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, ix_to_char)\n",
    "    if nb_epoch % 200 == 0:\n",
    "        model.save_weights('nametext_cps/checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
